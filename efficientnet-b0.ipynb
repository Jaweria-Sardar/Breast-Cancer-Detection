{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport timm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nimport seaborn as sns\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport logging\nimport cv2\n\n# Suppress albumentations update warning\nos.environ[\"NO_ALBUMENTATIONS_UPDATE\"] = \"1\"\n\n# Set up logging\nlogging.basicConfig(filename='training.log', level=logging.INFO, format='%(message)s')\n\n# Check environment\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA Version: {torch.version.cuda}\")\n    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Count: {torch.cuda.device_count()}\")\n\n# Set device (try GPU, fallback to CPU)\ntry:\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    if device.type == \"cuda\":\n        torch.cuda.set_device(0)\n        torch.cuda.empty_cache()\nexcept Exception as e:\n    print(f\"GPU initialization failed: {e}. Falling back to CPU.\")\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {device}\")\n\n# Custom Dataset for DDSM\nclass DDSMDataset(Dataset):\n    def __init__(self, data_source, transform=None, is_tfrecord=True):\n        self.transform = transform\n        self.is_tfrecord = is_tfrecord\n        self.data = []\n        self.labels = []\n        if is_tfrecord:\n            self._load_tfrecords(data_source)\n        else:\n            self._load_numpy(data_source)\n\n    def _load_tfrecords(self, tfrecord_files):\n        dataset = tf.data.TFRecordDataset(tfrecord_files)\n        invalid_labels = 0\n        invalid_images = 0\n        for raw_record in dataset:\n            try:\n                example = tf.train.Example()\n                example.ParseFromString(raw_record.numpy())\n                label = example.features.feature['label_normal'].int64_list.value[0]\n                if label not in [0, 1]:\n                    invalid_labels += 1\n                    continue\n                image_raw = example.features.feature['image'].bytes_list.value[0]\n                image = np.frombuffer(image_raw, dtype=np.uint8)\n                if image.size != 299 * 299:\n                    invalid_images += 1\n                    continue\n                image = image.reshape(299, 299, 1)\n                if np.any(np.isnan(image)) or not np.all(np.isfinite(image)):\n                    invalid_images += 1\n                    continue\n                self.data.append(image)\n                self.labels.append(int(label))\n            except Exception:\n                invalid_images += 1\n                continue\n        print(f\"Skipped {invalid_labels} samples with invalid labels, {invalid_images} with invalid images in tfrecords\")\n\n    def _load_numpy(self, data_source):\n        data_file, label_file = data_source\n        images = np.load(data_file)\n        labels = np.load(label_file)\n        invalid_images = 0\n        for img, lbl in zip(images, labels):\n            try:\n                if img.shape == (299, 299):\n                    img = img.reshape(299, 299, 1)\n                if np.any(np.isnan(img)) or not np.all(np.isfinite(img)):\n                    invalid_images += 1\n                    continue\n                binary_lbl = 0 if int(lbl) == 0 else 1\n                self.data.append(img)\n                self.labels.append(binary_lbl)\n            except Exception:\n                invalid_images += 1\n                continue\n        print(f\"Skipped {invalid_images} samples with invalid images in numpy files\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img = self.data[idx]\n        label = self.labels[idx]\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        if self.transform:\n            augmented = self.transform(image=img)\n            img = augmented['image']\n        return img, label\n\n# Define augmentations with additional transformations\ntrain_transform = A.Compose([\n    A.Resize(224, 224),\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\nval_transform = A.Compose([\n    A.Resize(224, 224),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ToTensorV2(),\n])\n\n# Load dataset\nDATA_DIR = \"/kaggle/input/ddsm-mammography\"\ntrain_files = [\n    os.path.join(DATA_DIR, f\"training10_{i}/training10_{i}.tfrecords\") for i in range(5)\n]\nval_data = (os.path.join(DATA_DIR, \"cv10_data/cv10_data.npy\"), os.path.join(DATA_DIR, \"cv10_labels.npy\"))\ntest_data = (os.path.join(DATA_DIR, \"test10_data/test10_data.npy\"), os.path.join(DATA_DIR, \"test10_labels.npy\"))\n\ntrain_dataset = DDSMDataset(train_files, transform=train_transform, is_tfrecord=True)\nval_dataset = DDSMDataset(val_data, transform=val_transform, is_tfrecord=False)\ntest_dataset = DDSMDataset(test_data, transform=val_transform, is_tfrecord=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=1)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=1)\n\nprint(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n\n# Compute class weights\nlabels = np.array([lbl for lbl in train_dataset.labels])\nprint(f\"Raw Labels: Min={np.min(labels)}, Max={np.max(labels)}, Unique={np.unique(labels)}\")\nif not np.all(np.isin(labels, [0, 1])):\n    raise ValueError(f\"Non-binary labels detected: {np.unique(labels)}\")\nclass_counts = np.bincount(labels, minlength=2)\nprint(f\"Class counts: Negative={class_counts[0]}, Positive={class_counts[1]}\")\nif class_counts[0] == 0 or class_counts[1] == 0:\n    raise ValueError(\"One class has zero samples, cannot compute class weights\")\ntotal_samples = len(labels)\nclass_weights = [total_samples / (2.0 * class_counts[i]) for i in range(2)]\nprint(f\"Class weights: {class_weights}\")\nif not all(np.isfinite(class_weights)):\n    raise ValueError(f\"Invalid class weights: {class_weights}\")\nclass_weights = torch.tensor(class_weights[1], dtype=torch.float32, device=device)\n\n# EfficientNet-B0 Model\nclass EfficientNetB0Classifier(nn.Module):\n    def __init__(self, num_classes=1):\n        super(EfficientNetB0Classifier, self).__init__()\n        try:\n            self.effnet = timm.create_model('efficientnet_b0', pretrained=True, num_classes=0)\n            print(\"Loaded efficientnet_b0 pretrained weights\")\n        except Exception as e:\n            print(f\"Failed to load efficientnet_b0 pretrained weights: {e}. Trying tf_efficientnet_b0...\")\n            try:\n                self.effnet = timm.create_model('tf_efficientnet_b0', pretrained=True, num_classes=0)\n                print(\"Loaded tf_efficientnet_b0 pretrained weights\")\n            except Exception as e2:\n                print(f\"Failed to load tf_efficientnet_b0 pretrained weights: {e2}. Using random init.\")\n                self.effnet = timm.create_model('efficientnet_b0', pretrained=False, num_classes=num_classes)\n        # Unfreeze the last block (block 7) and classifier\n        for name, param in self.effnet.named_parameters():\n            if \"blocks.6\" in name:  # Unfreeze block 6 (last block in EfficientNet-B0)\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n        # Custom classifier with dropout\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(self.effnet.num_features, num_classes)\n        )\n        for param in self.classifier.parameters():\n            param.requires_grad = True\n\n    def forward(self, x):\n        x = self.effnet(x)\n        return self.classifier(x)\n\n# Initialize model and optimizer\ntry:\n    model = EfficientNetB0Classifier(num_classes=1).to(device)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n    # Use different learning rates for backbone and classifier\n    optimizer = optim.Adam([\n        {'params': [p for n, p in model.effnet.named_parameters() if p.requires_grad], 'lr': 1e-5},\n        {'params': model.classifier.parameters(), 'lr': 1e-4}\n    ])\nexcept Exception as e:\n    print(f\"GPU model initialization failed: {e}. Falling back to CPU.\")\n    device = torch.device(\"cpu\")\n    model = EfficientNetB0Classifier(num_classes=1).to(device)\n    class_weights = class_weights.to(device)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n    optimizer = optim.Adam([\n        {'params': [p for n, p in model.effnet.named_parameters() if p.requires_grad], 'lr': 1e-5},\n        {'params': model.classifier.parameters(), 'lr': 1e-4}\n    ])\n\n# Training parameters\nepochs = 100\nbest_val_acc = 0\ntrain_acc, val_acc = [], []\ntrain_loss, val_loss = [], []\ntrain_auc, val_auc = [], []\n\n# Evaluation function\ndef evaluate_dl(loader, model, criterion):\n    model.eval()\n    correct, total, loss_total = 0, 0, 0\n    y_true, y_scores = [], []\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device).float().view(-1, 1)\n            if torch.any(torch.isnan(images)) or torch.any(torch.isinf(images)):\n                print(\"Invalid input images (NaN or inf)\")\n                continue\n            outputs = model(images)\n            if torch.any(torch.isnan(outputs)) or torch.any(torch.isinf(outputs)):\n                print(\"Invalid model outputs (NaN or inf)\")\n                continue\n            loss = criterion(outputs, labels)\n            predicted = (torch.sigmoid(outputs) >= 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            loss_total += loss.item()\n            y_true.extend(labels.cpu().numpy().flatten())\n            y_scores.extend(torch.sigmoid(outputs).cpu().numpy().flatten())\n    acc = correct / total if total > 0 else 0\n    auc = roc_auc_score(y_true, y_scores) if len(np.unique(y_true)) > 1 and len(y_true) > 0 else 0\n    return acc, loss_total / len(loader) if len(loader) > 0 else 0, auc, y_true, y_scores\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0\n    for i, (images, labels) in enumerate(train_loader):\n        try:\n            images, labels = images.to(device), labels.to(device).float().view(-1, 1)\n            if torch.any(torch.isnan(images)) or torch.any(torch.isinf(images)):\n                print(f\"Batch {i}: Invalid input images (NaN or inf)\")\n                continue\n            optimizer.zero_grad()\n            outputs = model(images)\n            if torch.any(torch.isnan(outputs)) or torch.any(torch.isinf(outputs)):\n                print(f\"Batch {i}: Invalid model outputs (NaN or inf)\")\n                continue\n            loss = criterion(outputs, labels)\n            if torch.isnan(loss) or torch.isinf(loss):\n                print(f\"Batch {i}: Invalid loss (NaN or inf)\")\n                continue\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            if i % 100 == 0:\n                print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}\")\n        except RuntimeError as e:\n            print(f\"GPU error in batch {i}: {e}. Falling back to CPU.\")\n            device = torch.device(\"cpu\")\n            model = model.to(device)\n            images, labels = images.to(device), labels.to(device)\n            class_weights = class_weights.to(device)\n            criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n            optimizer = optim.Adam([\n                {'params': [p for n, p in model.effnet.named_parameters() if p.requires_grad], 'lr': 1e-5},\n                {'params': model.classifier.parameters(), 'lr': 1e-4}\n            ])\n            continue\n\n    t_acc, t_loss, t_auc, _, _ = evaluate_dl(train_loader, model, criterion)\n    v_acc, v_loss, v_auc, _, _ = evaluate_dl(val_loader, model, criterion)\n    train_acc.append(t_acc)\n    val_acc.append(v_acc)\n    train_loss.append(t_loss)\n    val_loss.append(v_loss)\n    train_auc.append(t_auc)\n    val_auc.append(v_auc)\n    print(f\"Epoch {epoch+1}: Train Acc: {t_acc:.4f}, Val Acc: {v_acc:.4f}, Train AUC: {t_auc:.4f}, Val AUC: {v_auc:.4f}, Train Loss: {t_loss:.4f}, Val Loss: {v_loss:.4f}\")\n\n    if v_acc > best_val_acc:\n        best_val_acc = v_acc\n        torch.save(model.state_dict(), 'efficientnet_b0_best.pth')\n\n# Evaluate on Test set\nmodel.load_state_dict(torch.load('efficientnet_b0_best.pth', map_location=device, weights_only=True))\ntest_acc, test_loss, test_auc, y_true_test, y_scores_test = evaluate_dl(test_loader, model, criterion)\n\nprint(f\"\\nFinal Results:\")\nprint(f\"Test Acc: {test_acc:.4f}, AUC: {test_auc:.4f}, Loss: {test_loss:.4f}\")\n\n# Classification report\nprint(\"\\nTest Classification Report:\")\nprint(classification_report(y_true_test, (np.array(y_scores_test) >= 0.5).astype(int), target_names=['NEGATIVE', 'POSITIVE']))\n\n# Graphs\ncm = confusion_matrix(y_true_test, (np.array(y_scores_test) >= 0.5).astype(int))\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=['NEGATIVE', 'POSITIVE'], yticklabels=['NEGATIVE', 'POSITIVE'])\nplt.title(\"Confusion Matrix (Test Set)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.savefig('confusion_matrix.png')\nplt.show()\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(train_acc, label='Train Acc')\nplt.plot(val_acc, label='Val Acc')\nplt.title(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(train_loss, label='Train Loss')\nplt.plot(val_loss, label='Val Loss')\nplt.title(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.tight_layout()\nplt.savefig('training_history.png')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}